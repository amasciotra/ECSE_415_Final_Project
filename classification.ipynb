{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification.ipynb\n",
    "Date: November 29th, 2018  \n",
    "Course: ECSE415, McGill University  \n",
    "Authors:  \n",
    "*Shawn Vosburg  \n",
    "Tristan Bouchard  \n",
    "Alex Masciotra  \n",
    "Nayem Alam  \n",
    "Thomas Philippon *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: The classification files folder must be found in the same folder as this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named sklearn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-79bdc5227fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mrandom\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named sklearn"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries. Strategy: Find HoG features of each image and build SVM with them.\n",
    "#For second classifier, try Kmeans or Kneighboors\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import  random as rand\n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Declare Constants\n",
    "CLASSIFY_DIM = (128,128)   #Size of the training images\n",
    "KCROSSVALID_NUM = 10       #Number of bins when doing k-cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "### Loading and resizing images && Compute HOG features of all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoGFromImage(img,cs,bs,nb):\n",
    "    \"\"\" This function takes in an image array and HoG param and returns the computed histogram for the image array\n",
    "        img = image array. all images must be same resolution. \n",
    "        cs = cell size in pixel x pixel (height x width)\n",
    "        bc = bin size in cell x cell (height x width)\n",
    "        nb = number of bins\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # create HoG Object\n",
    "    # winSize is the size of the image cropped to an multiple of the cell size\n",
    "    hog = cv2.HOGDescriptor(_winSize=(img.shape[1] // cs[1] * cs[1],\n",
    "                                      img.shape[0] // cs[0] * cs[0]),\n",
    "                            _blockSize=(bs[1] * cs[1],\n",
    "                                        bs[0] * cs[0]),\n",
    "                            _blockStride=(cs[1], cs[0]),\n",
    "                            _cellSize=(cs[1], cs[0]),\n",
    "                            _nbins=nb)\n",
    "    \n",
    "    \n",
    "    n_cells = (img.shape[0] // cs[0], img.shape[1] // cs[1])\n",
    "        \n",
    "\n",
    "    # Compute HoG features\n",
    "    hog_feats = hog.compute(img)\\\n",
    "                   .reshape(n_cells[1] - bs[1] + 1,\n",
    "                            n_cells[0] - bs[0] + 1,\n",
    "                            bs[0], bs[1], nb) \\\n",
    "                   .transpose((1, 0, 2, 3, 4))  # index blocks by rows first\n",
    "\n",
    "    # hog_feats now contains the gradient amplitudes for each direction,for each cell of its group for each group.\n",
    "    # Indexing is by rows then columns.\n",
    "\n",
    "    # computation for BlockNorm\n",
    "    gradients = np.full((n_cells[0], n_cells[1], 8), 0, dtype=float)\n",
    "    cell_count = np.full((n_cells[0], n_cells[1], 1), 0, dtype=int)\n",
    "\n",
    "    #Add each contributions to the histogram.\n",
    "    for off_y in range(bs[0]):\n",
    "        for off_x in range(bs[1]):\n",
    "            gradients[off_y:n_cells[0] - bs[0] + off_y + 1,\n",
    "                      off_x:n_cells[1] - bs[1] + off_x + 1] += \\\n",
    "                hog_feats[:, :, off_y, off_x, :]\n",
    "            cell_count[off_y:n_cells[0] - bs[0] + off_y + 1,\n",
    "                       off_x:n_cells[1] - bs[1] + off_x + 1] += 1\n",
    "\n",
    "    # Average gradients\n",
    "    gradients /= cell_count\n",
    "    return gradients\n",
    "\n",
    "#Create function to save classifier\n",
    "def saveCLF(clf,path):\n",
    "    joblib.dump(clf,path)\n",
    "    \n",
    "#Create function to load classifier\n",
    "def loadCLF(path):\n",
    "    return joblib.load(path)\n",
    "\n",
    "#Class to hold the feature data and the label\n",
    "class labeledData():\n",
    "    def __init__(self,data,label,num):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.num = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training images\n",
    "folder = \"./MIO-TCD-Classification/train/\"\n",
    "vehicleName = [\n",
    "    \"articulated_truck\", \"background\", \"bicycle\",\n",
    "    \"bus\", \"car\", \"motorcycle\", \"non-motorized_vehicle\",\n",
    "    \"pedestrian\", \"pickup_truck\", \"single_unit_truck\", \"work_van\"                \n",
    "]\n",
    "vehicleDir  = [                                  #Main Directory of Training images\n",
    "    #\"articulated_truck\",        #10346 imgs\n",
    "    #\"background\",               #160000 imgs\n",
    "    \"bicycle\",                  #2284 imgs\n",
    "    \"bus\",                      #10316 imgs\n",
    "    #\"car\",                      #260518 imgs\n",
    "    \"motorcycle\",               #1982 imgs\n",
    "    \"non-motorized_vehicle\",    #1751 imgs\n",
    "    \"pedestrian\",               #6262 imgs\n",
    "    #\"pickup_truck\",             #50906 imgs\n",
    "    \"single_unit_truck\",        #5120 imgs\n",
    "    #\"work_van\"                 #9679 imgs\n",
    "]                 \n",
    "#Some vehicletypes are commented out as they take too much time to load. After writing code, will remove comment.\n",
    "\n",
    "vehicleTypes = {}                                #This is the main hash that will map the vehicles type to a number. Maps string to index.\n",
    "vehicleImgArr,vehicleLabels,hogArr = [],[],[]\n",
    "idx = 0\n",
    "\n",
    "#Build Hash Table\n",
    "for bucket in os.listdir(folder):\n",
    "    vehicleTypes[bucket] = idx\n",
    "    idx +=1\n",
    "\n",
    "#HOG features parameters\n",
    "cell_size = (16,16)  # h x w in pixels #Changed from 4x4 to 16x16 to speed up process. \n",
    "block_size = (2,2)  # h x w in cells\n",
    "nbins = 8  # number of orientation bins\n",
    "\n",
    "#Loop across directory to fetch all images\n",
    "start = time.time()\n",
    "for typeName in vehicleDir:\n",
    "    vehicleTypeDir = folder + typeName+\"/\"\n",
    "    print(\"Presently loading & finding HOG features of \" + str(len(os.listdir(vehicleTypeDir))) + \" images from: \" + typeName,end=\"\")\n",
    "    presentTime = time.time()\n",
    "    \n",
    "    #fetch images\n",
    "    for imgPath in os.listdir(vehicleTypeDir):\n",
    "        img = cv2.imread(vehicleTypeDir+imgPath)\n",
    "        img = cv2.resize(img,CLASSIFY_DIM)                          #Resize images so that they are all CLASSIFY_DIM in size\n",
    "        vehicleImgArr.append(img)                                  #Commented this out to save processing time. \n",
    "        vehicleLabels.append(vehicleTypes[typeName])                #Have a seperate array with the labels.\n",
    "        hogArr.append(HoGFromImage(img,cell_size,block_size,nbins)) #Find Hog Features\n",
    "    endTime = time.time()\n",
    "    print(\". (Time taken: \", endTime - presentTime,\"secs)\")\n",
    "hogArr = np.asarray(hogArr)\n",
    "vehicleLabels = np.asarray(vehicleLabels)\n",
    "end = time.time()\n",
    "\n",
    "#Print out report:\n",
    "print(\"REPORT: \\n\\tTotal Number of images:\",len(hogArr),\"\\n\\tTotal time taken: \",end-start,\"sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview of Image and its HOG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview image and its hog features\n",
    "preview = 1001\n",
    "print(\"Example of test image and its hog features:\")\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.subplot(191)\n",
    "plt.imshow(vehicleImgArr[preview])\n",
    "for bin in range(nbins):\n",
    "    plt.subplot(192+bin)\n",
    "    plt.pcolor(hogArr[preview][:, :, bin%nbins])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    title = \"HOG bin: \" +  str(bin%nbins)\n",
    "    plt.title(title), plt.xticks([]), plt.yticks([])\n",
    "    plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "All images are loaded inside *vehicleImgArr* with its HOG features being computed-&-loaded inside *hogArr*. The features that are calculted are of size (8,8,8) meaning that for 8 angular directions, 8x8 array of hog features were calculated for each images (see above for the preview of what sample image with its hog features look like). Therefore, for each image, features of size $8^3 = 512$ are calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier #1: SVM\n",
    "#### Creating classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building filepath\n",
    "savingPath = \"./Classifiers/SVM/\"\n",
    "clfFileType = \".clf\"\n",
    "name = \"SVMclf\"+ \"_Main\"\n",
    "path = savingPath + name +clfFileType\n",
    "\n",
    "#Build classifier if it doesn't already exist\n",
    "if(not os.path.isfile(path)):\n",
    "    #Creating main classifier that is trained with all training images. \n",
    "    dataSVM = hogArr.reshape(len(hogArr),-1)\n",
    "    labelsSVM = vehicleLabels.reshape(-1)\n",
    "\n",
    "    #Train SVM classifier\n",
    "    print(\"Building the main SVM classifier ...\",end = '')\n",
    "    pTime = time.time()\n",
    "    clfMain = svm.SVC(gamma=0.001, C=100.)\n",
    "    clfMain.fit(dataSVM, labelsSVM)\n",
    "    eTime = time.time()\n",
    "    print(\"DONE!\")\n",
    "    print(\"Time taken to build main SVM classifier:\",eTime-pTime,\"sec\")\n",
    "\n",
    "    #Saving main SVM classifier\n",
    "    saveCLF(clfMain,path)\n",
    "\n",
    "#If the file already exist, just load the classifier instead of building a new one (saves a lot of time)\n",
    "else:\n",
    "    clfMain = loadCLF(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "In order to avoid always rebuilding the same classifier everytime, logic has been added to load an already saved version of the calssifier. The main SVM classifier is saved as *./Classifiers/SVM/SVMclf_Main.clf*. To build a new classifier, please delete that file before running the jupyter cell above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier #2: Viola-Jones? Kmeans? K-NearestNeighboor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Classifier evaluation\n",
    "### 2.1.1 Evaluating SVM\n",
    "\n",
    "#### Creating classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING K-CROSSVALIDATION\n",
    "#BUILDING ARRAY OF SIZE K WITH ALL K-CLASSIFIERS \n",
    "\n",
    "#Reshape the data to be a NxD array\n",
    "data = hogArr.reshape(len(hogArr),-1)\n",
    "labels = vehicleLabels.reshape(len(vehicleLabels),-1)\n",
    "\n",
    "#create labeled data for shuffling if it isn't already saved; else used saved and already shuffled version\n",
    "savingPath = \"./Classifiers/SVM/\"\n",
    "dataFileType = \".data\"\n",
    "name = \"labeledData_Validation\" + str(KCROSSVALID_NUM)\n",
    "path = savingPath +name+ dataFileType\n",
    "if(not os.path.isfile(path)):\n",
    "    #Create a randomized array \n",
    "    print(\"Shuffling labeled data ...\",end = '')\n",
    "    dataAndLabel = [labeledData(data[i],labels[i],i) for i in range(len(data))]\n",
    "    dataAndLabel = np.asarray(dataAndLabel) \n",
    "    rand.shuffle(dataAndLabel)\n",
    "    print(\"DONE!\")\n",
    "    print(\"Saving labeled data ...\",end = '')\n",
    "    joblib.dump(dataAndLabel,path)\n",
    "    print(\"DONE!\")\n",
    "else:\n",
    "    print(\"Loading labeled data ...\",end = '')\n",
    "    dataAndLabel = joblib.load(path)\n",
    "    print(\"DONE!\")\n",
    "    \n",
    "#split data into training and testing arrays\n",
    "k = KCROSSVALID_NUM\n",
    "sizeOfBin = len(dataAndLabel) // k\n",
    "savingPath = \"./Classifiers/SVM/\"\n",
    "clfFileType = \".clf\"\n",
    "clfArr = []\n",
    "for kIdx in range(k):\n",
    "    name = \"SVMclf_Validation\"+ str(kIdx)\n",
    "    path = savingPath +name+ clfFileType\n",
    "    #Only create a new classifier if it hasn't been built before (implemented as such because they take a long time to generate)\n",
    "    if(not os.path.isfile(path)):\n",
    "        #Creating Training Array\n",
    "        labeledTrain = np.delete(dataAndLabel,np.arange(kIdx*sizeOfBin,(kIdx+1)*sizeOfBin))  #Size of (k-1)*sizeOfBin\n",
    "        Train = [labeledTrain[i].data for i in range(len(labeledTrain))]\n",
    "        Train = np.asarray(Train)\n",
    "        TrainLabel= [labeledTrain[i].label for i in range(len(labeledTrain))]\n",
    "        TrainLabel = np.asarray(TrainLabel).reshape(-1)\n",
    "\n",
    "        #Train SVM classifier\n",
    "        print(\"Building SVM classifier for testing k-bin =\",kIdx,\"...\",end = '')\n",
    "        pTime = time.time()\n",
    "        clf = svm.SVC(gamma=0.001, C=100.)\n",
    "        clf.fit(Train, TrainLabel)\n",
    "        clfArr.append(clf)\n",
    "        eTime = time.time()\n",
    "        print(\"DONE!\",end = \" \")\n",
    "        print(\"(Time taken to build classifier\",kIdx,\":\",eTime-pTime,\"sec)\")\n",
    "\n",
    "        #Saving SVM Classifier\n",
    "        name = \"SVMclf_Validation\"+ str(kIdx)\n",
    "        path = savingPath +name+ clfFileType\n",
    "        saveCLF(clf,path)\n",
    "    \n",
    "    #If classifier already exist, just print a statement saying it already exist\n",
    "    else:\n",
    "        print(\"Loading existing SVM classifier for k-bin =\", kIdx, \"...\",end=\"\")\n",
    "        pTime = time.time()\n",
    "        clfArr.append(loadCLF(path))\n",
    "        eTime = time.time()\n",
    "        print(\"DONE!\",end = \" \")\n",
    "        print(\"(Time taken to load classifier\",kIdx,\":\",eTime-pTime,\"sec)\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting important variables\n",
    "loadingPath = \"./Classifiers/SVM/\"\n",
    "clfFileType = \".clf\"\n",
    "dataFileType = \".data\"\n",
    "k = KCROSSVALID_NUM\n",
    "sizeOfBin = len(dataAndLabel) // k\n",
    "\n",
    "#loading randomized labeled data that was used to train classifiers. Used to make sure that training images aren't used for testing\n",
    "path = loadingPath + \"labeledData_Validation\" + str(k) + dataFileType\n",
    "dataAndLabel = joblib.load(path)\n",
    "\n",
    "#Predicting the Test data\n",
    "accuracyArr = []\n",
    "for kIdx in range(k):\n",
    "    #Loading K-cross validation classifier\n",
    "    clf = clfArr[kIdx]\n",
    "    \n",
    "    #Building test arrays\n",
    "    labeledTest  = dataAndLabel[kIdx*sizeOfBin:(kIdx+1)*sizeOfBin]  #Size of sizeofBin\n",
    "    Test = [labeledTest[i].data for i in range(len(labeledTest))]\n",
    "    Test = np.asarray(Test)\n",
    "    TestLabel= [labeledTest[i].label for i in range(len(labeledTest))]\n",
    "    TestLabel = np.asarray(TestLabel).reshape(-1)\n",
    "    \n",
    "    #Predicting the test array\n",
    "    print(\"Predicting labels of Test images from k-bin =\",kIdx,\"...\",end = '')\n",
    "    start = time.time()\n",
    "    predicted = clf.predict(Test)\n",
    "    end = time.time()\n",
    "    print(\"DONE!\")\n",
    "    print(\"Time taken to predict images: \",end-start,\"sec\")\n",
    "\n",
    "    #Analizing the predicted labels\n",
    "    totalAccuracy = predicted - TestLabel\n",
    "    accuracy = len(total[np.where(totalAccuracy == 0)])/len(totalAccuracy)\n",
    "    accuracyArr.append(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating SVM classifier\n",
    "#### Average classification accuracy across validations, with the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating report of accuracy of all validations\n",
    "accuracyArr = np.asarray(accuracyArr)\n",
    "print(\"REPORT ACCURACY: \")\n",
    "print(\"\\tMean of ratios of erroneously classified images:\",np.mean(accuracyArr))\n",
    "print(\"\\tStandard deviation of ratios of erroneously classified images:\",np.std(accuracyArr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average precision* and recall* across validations. Are these values consistent with the accuracy? Are they more representative of the dataset? In what situations would you expect precision and recall to be a better reflection of model performance than accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A confusion matrix on a validation set. Plot the matrix as an image. Are any of the classes difficult for your classifier? Discuss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
